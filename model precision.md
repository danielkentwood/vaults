# model precision 
parent:: [[model evaluation metrics]]

Precision is the ratio of TP, the true positives (i.e., correct labeling of positive observations) versus $\hat{\text{P}}$, the number of observations that the model labeled as positive. 

It is also called the *positive predictive value*, or the *True Positive Rate*.


$$
\text{TPR}=\frac{\text{TP}}{\hat{\text{P}}}
$$



![F1%20Score%2013829d2ecac54e43a0902525fe68164a/confusion_matrix.png](confusion_matrix.png)
